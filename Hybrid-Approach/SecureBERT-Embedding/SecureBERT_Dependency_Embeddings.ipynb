{"cells":[{"cell_type":"markdown","metadata":{"id":"uhJYqiWdn9pn"},"source":["This notebook only contains code for the SecureBERT hybrid embeddings and NOT the clustering."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbfUMzdgn0j7","executionInfo":{"status":"ok","timestamp":1761337423321,"user_tz":360,"elapsed":29308,"user":{"displayName":"Josh Schultze","userId":"02185956454341759681"}},"outputId":"d9362fe8-8e8f-42eb-a556-7b371ba9e29c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["import re # parsing for converting strings of tensors to arrays\n","import pandas as pd\n","%pip install torch\n","import torch\n","import torch.nn.functional as F\n","from torch import Tensor\n","import transformers\n","from transformers import RobertaTokenizerFast, RobertaModel\n","import numpy as np\n","# Initalize model\n","model_name = \"ehsanaghaei/SecureBERT\"\n","tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n","model= RobertaModel.from_pretrained(model_name, add_pooling_layer=False).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1761337423466,"user":{"displayName":"Josh Schultze","userId":"02185956454341759681"},"user_tz":360},"id":"97P-NDS0iCKs","outputId":"4324cbcf-98a8-4f19-8458-38e8657abd94"},"outputs":[{"output_type":"stream","name":"stdout","text":["   article_id                                         event_text\n","0           0  attain semblance ; be desire ; be group ; be s...\n","1           1  access license ; ai move ; build team ; certif...\n","2           2  2020 be ; actor cripple time ; actor move void...\n","3           3  adapt threat ; anticipate detect ; anticipate ...\n","4           4  apply patch ; be dependency ; charge waste ; c...\n"]}],"source":["df = pd.read_csv(\"article_event_templates.csv\", encoding=\"latin1\")\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1188,"status":"ok","timestamp":1761337424659,"user":{"displayName":"Josh Schultze","userId":"02185956454341759681"},"user_tz":360},"id":"wz8-dbbXxwNy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f892a1d6-be0b-412b-de22-1c242c7855ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["   orig_row  article_id        split_text  within_row_id\n","0         0           0  attain semblance              0\n","1         0           0         be desire              1\n","2         0           0          be group              2\n","3         0           0         be source              3\n","4         0           0        benefit be              4\n","   article_id        split_text  triple_id\n","0           0  attain semblance          0\n","1           0         be desire          1\n","2           0          be group          2\n","3           0         be source          3\n","4           0        benefit be          4\n"]}],"source":["# split into lists (strip optional braces and surrounding spaces)\n","df[\"chunks\"] = df[\"event_text\"].str.strip(\"{}\").str.split(r\"\\s*;\\s*\", regex=True)\n","\n","# explode and add within-row ids (based on the original row index)\n","out = (\n","    df[[\"article_id\", \"chunks\"]]\n","      .explode(\"chunks\")\n","      .rename(columns={\"chunks\": \"split_text\"})\n","      .dropna()\n","      .assign(within_row_id=lambda d: d.groupby(level=0).cumcount())  # 0,1,2,...\n","      .reset_index(names=\"orig_row\")  # original row number\n",")\n","\n","print(out.head())\n","\n","#Creating a streamlined data frame\n","df = out\n","df = df.rename(columns={\"within_row_id\": \"triple_id\"})\n","df = df.drop(columns=[\"orig_row\"])\n","\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"CrFZ44NFzpTg"},"source":["# Loading in SecureBERT and then applying embedding to every text"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":493,"status":"ok","timestamp":1761337425155,"user":{"displayName":"Josh Schultze","userId":"02185956454341759681"},"user_tz":360},"id":"66jyPPzszeTG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4631f55-1f23-466e-f7c3-21bf99953bee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is the final shape of the embedding object: torch.Size([1, 768])\n","Here is an example of the embedding object in full\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.5009e-02, -2.7689e-02, -4.9516e-03, -6.9602e-03,  3.7658e-02,\n","          4.5381e-02, -1.4163e-02,  9.1673e-03, -6.7716e-03, -1.5828e-02,\n","         -1.7022e-02,  2.4589e-02, -8.7878e-03,  1.5037e-02,  2.7817e-02,\n","          7.2541e-02,  2.5042e-02, -1.4167e-02,  1.1876e-02,  9.4069e-02,\n","         -4.3695e-02,  3.2139e-02, -1.5925e-02, -7.8477e-03,  2.3173e-02,\n","          3.3872e-02, -3.3681e-02, -5.1435e-03, -1.4858e-02,  1.7103e-02,\n","         -1.1382e-02,  7.3876e-03, -1.2634e-02,  1.9167e-02,  2.0603e-02,\n","          2.6289e-02, -1.6248e-02, -5.4231e-03,  8.8549e-02,  3.7792e-03,\n","          3.0797e-02, -8.2246e-02,  1.1658e-03, -1.9300e-03, -6.9374e-02,\n","          1.3297e-02,  3.9672e-03, -2.4561e-02,  7.6257e-03,  5.7704e-03,\n","         -2.2682e-02,  3.2992e-02, -2.8369e-03,  2.5972e-02, -5.3120e-02,\n","         -1.0593e-02, -2.3154e-03,  3.7218e-02, -1.1401e-02,  2.8248e-02,\n","          1.9864e-02,  1.2499e-01, -6.6121e-02,  3.1625e-02,  3.0383e-03,\n","         -4.1395e-02, -1.5580e-02, -1.8642e-02,  2.1359e-02, -4.8618e-03,\n","          8.8803e-03,  1.4215e-02, -3.8942e-03, -2.8096e-02,  4.5370e-02,\n","          6.8312e-02, -5.5410e-03, -3.8983e-01, -1.9196e-02,  2.6418e-02,\n","          1.2480e-02, -5.0874e-02,  3.1551e-01, -1.3699e-02,  4.6911e-02,\n","          2.4196e-02,  1.1983e-02, -1.2007e-02,  1.4959e-02,  1.3880e-02,\n","          3.0416e-02, -9.0324e-03, -1.1783e-02,  4.2809e-02,  1.0336e-02,\n","          4.4915e-02, -1.3444e-02,  1.0613e-01,  2.0025e-02, -3.4471e-02,\n","          1.6951e-02, -7.8825e-03,  4.6470e-02,  1.4847e-02, -9.0142e-03,\n","          7.4160e-03, -7.1202e-02, -2.4145e-02, -3.6003e-02,  3.0687e-02,\n","          8.8634e-03,  1.0396e-02,  3.9583e-02, -1.8419e-02, -5.3331e-02,\n","          2.2630e-02,  5.2080e-03,  3.6376e-03,  2.3324e-02,  5.0044e-02,\n","          4.1585e-02,  4.5003e-03, -1.8074e-02,  1.4481e-02,  2.7973e-02,\n","          5.3806e-02, -4.6002e-03,  1.0800e-02, -1.6715e-02,  2.7199e-02,\n","          3.3746e-02, -2.1435e-02, -1.5621e-02,  2.3981e-02,  1.5679e-02,\n","         -2.3122e-02,  4.3490e-02, -3.9744e-02, -1.3825e-02, -1.7968e-02,\n","         -1.8849e-02,  4.1921e-03,  1.2216e-02,  1.1155e-03,  4.2875e-02,\n","          1.1390e-02,  8.4992e-03, -3.2751e-02, -2.1522e-02,  3.0691e-02,\n","          2.5352e-02, -2.7305e-02, -2.1292e-02,  3.3152e-03, -2.2506e-02,\n","         -9.7452e-03, -3.4223e-02,  8.8122e-02, -7.1480e-03,  4.5840e-02,\n","          1.6691e-02, -2.2369e-02, -6.2887e-03, -1.7502e-02, -2.5769e-02,\n","         -1.5397e-02, -8.7748e-03,  5.0316e-03, -1.1797e-02,  4.6211e-02,\n","         -2.0007e-02,  2.3103e-02, -3.5347e-02, -1.1506e-02,  3.1361e-02,\n","         -3.2581e-03, -2.7066e-02,  3.1076e-02,  3.5067e-02,  1.8388e-02,\n","         -2.4517e-02, -1.5885e-02, -6.2254e-03, -4.6210e-02, -8.2482e-03,\n","          5.9375e-02,  4.3325e-02, -4.2171e-02,  3.1142e-02,  4.6435e-02,\n","          4.6169e-02,  4.1961e-02,  2.4345e-02,  9.9193e-02, -2.8862e-02,\n","         -9.8268e-03,  1.8886e-03,  3.5816e-03, -1.9027e-02, -1.9361e-02,\n","          4.9717e-03, -7.0853e-03,  4.4794e-02, -1.9946e-02,  3.2868e-02,\n","          1.3596e-02,  6.8308e-03,  2.4424e-02, -1.3888e-02, -2.7918e-02,\n","         -1.6912e-02,  1.9381e-03,  1.3958e-03, -6.6648e-03,  1.7040e-02,\n","         -4.5732e-02,  1.2380e-03, -3.5063e-02,  2.8505e-02,  6.5615e-02,\n","         -3.4077e-02, -3.4967e-03,  4.0959e-03, -1.6443e-02, -2.8975e-02,\n","          3.7203e-02, -8.4126e-03, -6.2114e-03,  2.2077e-02,  3.2759e-02,\n","          6.2553e-03,  2.7311e-02, -3.6425e-02,  6.4108e-03,  1.6737e-02,\n","         -7.6422e-02, -1.4946e-02,  3.1408e-02,  2.8152e-03,  9.7942e-03,\n","         -9.7217e-02,  3.7031e-03,  3.1034e-02,  2.7460e-02, -1.8796e-02,\n","         -6.0443e-03, -3.8030e-02,  7.6180e-02,  1.2564e-02, -1.7747e-02,\n","         -2.1570e-02, -1.1942e-02,  5.1681e-04, -2.9007e-02, -1.3325e-02,\n","         -4.1629e-03, -8.0283e-04,  3.4717e-04, -4.8091e-03, -5.5077e-02,\n","          2.8778e-02,  3.8567e-02, -4.5784e-02, -1.0037e-02, -1.5685e-02,\n","         -8.2579e-03,  2.9409e-02,  2.5853e-02, -9.3901e-03, -3.7244e-02,\n","         -3.0354e-02,  5.8847e-03,  6.0026e-03,  3.1745e-03, -8.5019e-04,\n","         -1.4010e-02,  1.0462e-02,  1.7245e-02,  1.4454e-02,  2.5041e-02,\n","          1.9397e-02,  1.4797e-02, -7.5385e-03,  7.8291e-03,  9.4979e-03,\n","          3.1753e-03, -5.3323e-02, -1.3242e-02, -4.9818e-03, -2.1601e-02,\n","          4.7876e-02,  3.4165e-02,  1.0890e-02, -2.1987e-02, -5.0919e-02,\n","          3.4794e-02,  1.4723e-02, -6.3911e-03, -1.2324e-03, -1.9734e-02,\n","          7.5520e-03,  2.1698e-02,  2.4408e-03, -3.1608e-02, -4.1084e-03,\n","         -1.1899e-02,  3.2857e-02, -7.2535e-03, -6.0446e-04,  1.5618e-02,\n","         -2.3137e-04,  2.9261e-02,  6.2001e-03,  9.9750e-03,  5.5614e-03,\n","          2.0655e-02,  7.7360e-03, -2.1419e-02,  4.9392e-02,  3.7387e-02,\n","         -5.3864e-03, -4.0899e-02, -3.7114e-03,  7.3282e-03,  2.8099e-02,\n","          2.3085e-02, -3.1581e-02, -7.1811e-03,  7.8566e-03, -1.9271e-02,\n","         -2.9111e-01,  2.0271e-01, -4.4039e-03,  3.2969e-02, -3.3472e-02,\n","          1.8040e-02,  1.5327e-02,  3.7361e-03,  3.9089e-02, -5.8370e-03,\n","         -2.5039e-02, -2.8207e-02, -3.2420e-02,  4.5983e-03, -1.7901e-02,\n","          3.1107e-02, -8.5399e-04, -1.6398e-02,  5.2731e-03,  7.3096e-03,\n","          3.4084e-02, -1.6172e-02,  4.6656e-02, -7.9640e-03, -2.1392e-02,\n","          5.2096e-03,  3.6726e-02, -6.3741e-03,  1.2593e-02, -1.3360e-02,\n","          5.7854e-02,  6.2920e-02, -2.8364e-03,  9.0002e-03,  1.8656e-02,\n","         -4.1944e-02, -2.6375e-03,  2.2876e-02, -3.0863e-02, -3.4371e-02,\n","          2.1065e-02, -3.9629e-02, -2.4275e-02, -2.1736e-02, -1.1373e-02,\n","         -2.3104e-02, -2.6429e-02, -1.0090e-02,  1.0323e-02,  5.0433e-02,\n","          3.2706e-02,  8.5581e-03, -3.5787e-02,  2.2999e-02, -3.4674e-03,\n","         -5.3722e-03, -1.6138e-02, -6.3894e-03, -4.6663e-02, -4.4454e-03,\n","         -6.6392e-03,  9.6426e-03,  5.4486e-03, -1.4783e-02,  8.8944e-03,\n","         -1.1808e-02, -3.3423e-02, -2.7998e-02, -7.0749e-03, -1.1543e-02,\n","         -7.4889e-03,  3.3133e-02, -2.2703e-02,  2.6522e-02, -2.3190e-02,\n","         -5.3306e-02,  4.3027e-03,  9.6269e-03,  9.1168e-03, -2.7756e-02,\n","         -2.6044e-02,  1.9086e-02,  1.8041e-02, -3.3029e-02, -1.7735e-02,\n","         -2.3947e-02,  6.1641e-03,  1.0445e-03,  7.6031e-03,  4.7809e-03,\n","          2.7757e-02, -2.1203e-02,  2.0261e-02, -2.9978e-03, -6.5044e-04,\n","         -6.1611e-03,  1.6488e-02, -1.7742e-02, -1.1129e-02, -8.1423e-04,\n","          1.7869e-02,  7.3006e-04,  6.4165e-02,  2.4075e-03,  3.5962e-02,\n","         -2.0238e-02,  2.0421e-03,  4.2362e-04, -9.5223e-03, -1.7422e-02,\n","         -1.0505e-02,  3.3009e-02,  3.1423e-02, -1.7560e-04,  1.2531e-02,\n","          5.8787e-03,  5.3128e-03, -2.6607e-02, -2.3406e-03, -3.2412e-02,\n","          2.7627e-02,  1.1282e-02,  2.9331e-02, -3.8107e-01, -7.8298e-03,\n","          3.6785e-02, -1.3843e-02,  3.6467e-04,  3.3827e-02,  4.7118e-03,\n","         -1.0863e-02,  3.7198e-03, -3.8789e-02,  1.5970e-02, -5.4230e-02,\n","          2.3877e-03,  1.5160e-02,  4.4257e-02, -2.8603e-02, -8.8202e-03,\n","          2.2034e-02, -1.4162e-02, -1.4197e-02,  1.0623e-03, -3.5248e-02,\n","         -2.6939e-03,  9.2170e-03,  2.8487e-02,  3.6860e-02, -8.1552e-03,\n","          3.6304e-03,  1.7845e-02,  4.5337e-03, -1.1571e-03,  4.8294e-03,\n","         -7.5425e-03, -4.2568e-02, -1.2948e-02,  2.6353e-02, -1.2826e-02,\n","          1.9784e-02, -3.0791e-02, -2.7466e-02,  2.6324e-04, -4.1375e-02,\n","          2.6461e-02, -1.6198e-02, -8.9595e-03,  8.3537e-02, -1.4501e-02,\n","          1.4017e-02, -4.9044e-04,  2.0558e-02,  2.4811e-02,  2.0247e-02,\n","          1.0719e-02,  1.0361e-02, -2.1673e-02,  2.8602e-02,  4.6759e-02,\n","          7.7417e-05,  5.4932e-02,  1.4279e-02, -2.5713e-02, -1.9333e-02,\n","          3.0571e-02, -4.2413e-02,  2.8606e-02, -6.8894e-02,  6.1129e-02,\n","         -3.4970e-02,  1.8085e-02,  2.6064e-02, -1.6148e-02, -4.4765e-03,\n","         -1.2221e-02,  4.2429e-02, -1.0769e-02,  5.1645e-02,  8.6160e-03,\n","          6.4574e-02, -1.4577e-02,  2.2952e-02,  1.9064e-02,  2.5081e-02,\n","          8.2199e-04, -7.8073e-03,  5.4638e-02,  1.1382e-02, -2.3540e-03,\n","         -5.2735e-02,  1.4247e-02,  1.0690e-02, -1.2169e-02,  7.0270e-02,\n","         -2.4413e-02, -6.6348e-03, -4.6434e-03,  2.4815e-02, -4.3682e-02,\n","         -2.7300e-02, -1.5573e-01, -6.3019e-03,  3.7585e-02, -1.5173e-02,\n","         -3.6123e-03, -7.7007e-03, -1.2868e-02, -1.0100e-02, -1.8089e-03,\n","         -3.3175e-03, -1.5205e-02,  2.7557e-02,  2.5413e-02, -1.0117e-03,\n","         -4.5368e-03,  3.2606e-02,  6.1907e-03,  1.7534e-02,  2.4767e-02,\n","         -1.0575e-01, -1.6391e-03, -5.1454e-02,  1.5477e-02,  4.6166e-02,\n","         -7.5457e-03, -1.3702e-02,  2.8498e-02, -2.8839e-02, -7.0258e-03,\n","         -8.5300e-05, -9.4964e-03, -8.6874e-04, -4.2198e-03, -4.0893e-02,\n","          5.1090e-02,  2.7193e-02, -8.8345e-03,  1.5759e+00,  4.1015e-03,\n","         -2.2447e-02,  1.5000e-02,  1.3338e-02,  5.3717e-03,  1.2911e-02,\n","          6.1217e-03,  3.2396e-02,  2.1974e-02,  4.0544e-03,  8.2210e-03,\n","          1.1459e-02, -1.5253e-02,  4.4627e-02,  1.6516e-03, -5.9245e-02,\n","         -1.5826e-02,  7.4195e-03,  2.1058e-02,  3.1274e-02,  1.4394e-02,\n","         -1.7927e-02,  8.5248e-02, -1.4630e-02,  3.5726e-02,  3.1139e-02,\n","          2.7325e-02, -1.3134e-02, -1.0690e-02, -2.8808e-02,  1.2081e-02,\n","          1.5438e-02, -2.7226e-03,  4.5997e-02,  3.1000e-02,  3.8098e-02,\n","          1.4509e-02, -1.5493e-02, -4.1283e-03,  2.8092e-02,  5.7452e-03,\n","          6.1532e-03,  1.5689e-02,  3.9299e-03, -8.7992e-03,  2.9773e-02,\n","          2.1816e-02,  2.2570e-02,  3.0197e-03, -4.4651e-03, -1.0656e-02,\n","          4.3145e-02, -1.2247e-02,  7.1624e-03, -2.0473e-03, -6.0751e-03,\n","          7.2034e-03,  1.2428e-02, -1.3579e-02,  2.0152e-02, -1.9777e-02,\n","          1.0389e-02,  5.9398e-02,  3.7650e-02,  2.5138e-02, -3.2251e-03,\n","          1.2786e-02,  4.1833e-02,  7.0217e-03, -2.7751e-02,  1.3293e-03,\n","         -2.9763e-02, -4.9556e-03,  2.1024e-02, -4.5726e-02, -3.3653e-03,\n","         -2.0730e-02, -1.2539e-02,  2.3349e-02,  3.0760e-02,  5.9289e-02,\n","          1.1082e-03, -2.2457e-03,  7.2737e-02,  4.7760e-02,  4.7872e-02,\n","         -1.4753e-02, -2.8927e-02, -2.4655e-03,  1.1247e-02, -2.8107e-02,\n","          4.7883e-02, -3.7105e-02,  4.5764e-02, -9.1286e-03, -2.0802e-02,\n","         -1.8269e-02,  6.4848e-03,  2.4687e-02, -8.0515e-03, -2.5097e-02,\n","          1.8051e-02, -8.7285e-03,  1.1665e-04,  6.2050e-03,  1.2961e-02,\n","         -4.6112e-03,  9.6301e-03, -1.0198e-02,  3.4707e-02, -1.2608e-02,\n","          4.0500e-02, -3.3434e-02,  3.4407e-02, -2.2335e-02, -8.2812e-03,\n","          9.8368e-03, -9.7427e-03,  1.4318e-02, -1.7957e-02,  1.1945e-02,\n","         -2.9167e-02,  1.7390e-02, -2.6310e-02,  3.5001e-03, -1.3747e-02,\n","         -3.8146e-02,  3.3492e-02,  1.5128e-02, -2.2240e-02,  1.9738e-02,\n","          5.2015e-02, -7.6779e-03, -3.4765e-03, -1.5838e-02,  2.9291e-02,\n","          2.2657e-03, -7.5152e-03, -3.7895e-05,  2.3664e-02, -1.3551e-02,\n","          2.8372e-02, -1.6774e-01,  2.9540e-02, -1.0676e-02,  6.8533e-02,\n","          3.5413e-02,  3.2578e-03,  2.7270e-02, -1.2160e-02, -3.7780e-02,\n","          3.1935e-02, -2.5692e-02,  3.2968e-02,  2.2169e-02, -3.3854e-02,\n","         -1.4587e-03, -2.8329e-03, -1.2300e-02, -5.0557e-02, -5.4892e-02,\n","          4.9107e-02, -3.2576e-02,  5.8013e-03, -1.6553e-04,  3.3254e-02,\n","          5.1798e-03,  8.4047e-04, -3.7096e-03, -1.7436e-02, -4.9544e-03,\n","          1.7316e-02, -3.5615e-02,  9.4095e-03,  5.4802e-03,  6.6158e-02,\n","         -1.0520e-02, -2.2851e-02, -2.7171e-03]])"]},"metadata":{},"execution_count":4}],"source":["\n","\n","\n","#This function will embed the text using SecureBERT\n","\n","## Commented out original embedding function:\n","# def SecureBERT_embed(article_text):\n","      # This puts the text into tokens wihtout padding or truncation! So chunk sizes need to be correct beforehand.\n"," # batch = tokenizer(article_text, return_tensors=\"pt\", padding=False, truncation=False)\n"," # with torch.no_grad():\n"," #     out = model(**batch)  # last_hidden_state: [1, T, H]. This is\n"," # last_hidden = out.last_hidden_state         # list is per-token contextual embeddings\n","#  sent_emb = last_hidden.mean(dim=1)          # averaging all token embeddings...\n","        # Did NOT normalize here since Anya does it in the HBDSCAN code.\n"," # return(sent_emb)\n","\n","def SecureBERT_embed(article_text):\n","    batch = tokenizer(article_text, return_tensors=\"pt\", padding=False, truncation=False)\n","    with torch.no_grad():\n","        out = model(**batch, output_hidden_states=True)\n","\n","    # out.hidden_states is a tuple: [embeddings, layer1, ..., layerN]\n","    hidden_states = out.hidden_states                # len = N_layers + 1\n","    last4 = hidden_states[-4:]                       # take last 4 layers\n","    # Average across the 4 layers -> [1, T, H]\n","    token_reps = torch.stack(last4, dim=0).mean(dim=0)\n","\n","    # Mean-pool across tokens with attention mask\n","    attn = batch[\"attention_mask\"].unsqueeze(-1).to(token_reps.dtype)  # [1, T, 1]\n","    masked = token_reps * attn                                         # zero out pads (if any)\n","    sent_emb = masked.sum(dim=1) / attn.sum(dim=1).clamp(min=1e-9)     # [1, H]\n","          # No normalization\n","\n","    return sent_emb\n","\n","\n","print(f\"Here is the final shape of the embedding object: {SecureBERT_embed(text).shape}\")\n","\n","print(\"Here is an example of the embedding object in full\")\n","SecureBERT_embed(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9I14U0vVzxkx","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1761344227235,"user_tz":360,"elapsed":6802077,"user":{"displayName":"Josh Schultze","userId":"02185956454341759681"}},"outputId":"4a1a455c-2763-4c07-fd91-50da2f6be3e3"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1692566346.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"triple_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSecureBERT_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2429346673.py\u001b[0m in \u001b[0;36mSecureBERT_embed\u001b[0;34m(article_text)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# last_hidden_state: [1, T, H]. This is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mlast_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m         \u001b[0;31m# list is per-token contextual embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0msent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# averaging all token embeddings...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["df[\"triple_embeddings\"] = df[\"split_text\"].apply(SecureBERT_embed)\n","print(df.head())\n","\n","#safeguard if the average doesn't work\n","article_securebert_embeddings_no_average_df =  df\n","article_securebert_embeddings_no_average_df.to_csv(\"SecureBERT_dependency_embeddings_no_average.csv\", index=False)\n","from google.colab import files\n","files.download(\"SecureBERT_dependency_embeddings_no_average.csv\")"]},{"cell_type":"code","source":["# Average over each article to get one embedding.\n","\n","print(df.head())\n","\n","\n","def mean_vec(series):\n","    arrs = [np.asarray(x, dtype=float) for x in series if isinstance(x, np.ndarray)]\n","    return np.mean(np.vstack(arrs), axis=0)\n","\n","\n","\n","## Anya added new func to convert str --> tensor embeddings --> numpy arrays to take the mean ##\n","# Why? triple_embeddings was saved as 'tensor[((# # # #....))]'\n","# just to be save, go thru cases -->\n","\n","\n","\n","def parse_embedding(x):\n","    # If it's already a tensor -> convert to numpy\n","    if isinstance(x, torch.Tensor):\n","        return x.detach().cpu().numpy()\n","\n","    # If it's a numpy array -> return as is\n","    if isinstance(x, np.ndarray):\n","        return x\n","\n","    # If it's a string -> attempt parsing\n","    if isinstance(x, str):\n","        # Remove the 'tensor([...])' wrapper\n","        cleaned = x.strip().replace(\"tensor(\", \"\").rstrip(\")\")\n","        # Convert string of numbers -> numpy\n","        return np.array(eval(cleaned), dtype=float)\n","\n","    raise TypeError(f\"Unknown embedding type: {type(x)}\")\n","\n","\n","def mean_vec(series):\n","    arrs = [np.asarray(x, dtype=float) for x in series]\n","    return np.mean(np.vstack(arrs), axis=0)\n","\n","df[\"triple_embeddings\"] = df[\"triple_embeddings\"].apply(parse_embedding)\n","\n","## Anya changed split_articles to triple_embeddings ##\n","article_securebert_embeddings_df = (df\n","            # .groupby(\"article_id\")[\"split_articles\"]\n","            .groupby(\"article_id\")[\"triple_embeddings\"]\n","            .apply(mean_vec)\n","            .reset_index(name=\"article_securebert_mean\"))\n","\n","\n","#check that it is the length of how many articles we have\n","print(article_securebert_embeddings_df.head())\n","len(article_securebert_embeddings_df)\n"],"metadata":{"id":"WvT-AFcjskcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save to csv\n","article_securebert_embeddings_df.to_csv(\"SecureBERT_dependency_embeddings.csv\", index=False)\n","from google.colab import files\n","files.download(\"SecureBERT_dependency_embeddings.csv\")"],"metadata":{"id":"0B4K2UdwWgdD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}